{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5813f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1: Importações de Bibliotecas e Módulos\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import impute, model_selection\n",
    "from sklearn.experimental import enable_iterative_imputer # Necessário para o IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2: Carga de Dados e Inspeção\n",
    "df = pd.read_excel('titanic3.xls')\n",
    "\n",
    "# Inspeção inicial (opcional, mas bom para checagem)\n",
    "print(\"Formato dos dados:\", df.shape)\n",
    "print(\"Cabeçalho:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aaf6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3: Definições das Funções do Pipeline (COM CORREÇÃO DO WARNING)\n",
    "\n",
    "def tweak_titanic(df):\n",
    "    \"\"\"Realiza a limpeza inicial e o one-hot encoding.\"\"\"\n",
    "    cols_to_drop = [\n",
    "        'name', 'ticket', 'home.dest', 'boat', 'body', 'cabin',\n",
    "    ]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Preenche NaNs na coluna 'embarked' (Categórica) com a Moda antes de fazer o get_dummies\n",
    "    df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
    "    \n",
    "    df = pd.get_dummies(df, drop_first=True)\n",
    "    return df\n",
    "\n",
    "def get_train_test_X_y(df, y_col, size=0.3, std_cols=None):\n",
    "    \"\"\"Divide, Imputa (Iterative) e Escala os dados de treino e teste.\"\"\"\n",
    "    \n",
    "    y = df[y_col]\n",
    "    X = df.drop(columns=y_col)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, y, test_size=size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Colunas que serão imputadas com IterativeImputer (apenas as de fato numéricas)\n",
    "    num_cols_to_impute = ['age', 'fare'] \n",
    "    \n",
    "    # 1. Imputação de Valores Ausentes\n",
    "    fi = impute.IterativeImputer(random_state=42)\n",
    "    X_train.loc[:, num_cols_to_impute] = fi.fit_transform(X_train[num_cols_to_impute])\n",
    "    X_test.loc[:, num_cols_to_impute] = fi.transform(X_test[num_cols_to_impute])\n",
    "    \n",
    "    # 2. Preenchimento de qualquer NaN remanescente com a mediana\n",
    "    meds = X_train.median()\n",
    "    X_train = X_train.fillna(meds)\n",
    "    X_test = X_test.fillna(meds)\n",
    "    \n",
    "    # 3. Escalonamento (BLOCO CORRIGIDO AQUI)\n",
    "    if std_cols:\n",
    "        std = StandardScaler()\n",
    "        # Converte para float antes da atribuição para evitar o FutureWarning\n",
    "        X_train.loc[:, std_cols] = std.fit_transform(X_train[std_cols].astype(float))\n",
    "        X_test.loc[:, std_cols] = std.transform(X_test[std_cols].astype(float))\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6dc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4: Aplicação do Pipeline e Divisão Final\n",
    "\n",
    "# 1. Aplica a limpeza e o encoding\n",
    "ti_df = tweak_titanic(df) \n",
    "\n",
    "# 2. Colunas para Escalonamento\n",
    "std_cols = \"pclass age sibsp fare\".split() \n",
    "\n",
    "# 3. Executa a divisão, imputação e escalonamento\n",
    "X_train, X_test, y_train, y_test = get_train_test_X_y(\n",
    "    ti_df, \"survived\", std_cols=std_cols\n",
    ")\n",
    "\n",
    "# Checagem final (opcional)\n",
    "print(\"\\nFormato do conjunto de Treino:\", X_train.shape)\n",
    "print(\"Valores ausentes em X_train:\", X_train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bbf46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Modelagem e Avaliação (Modelo Baseline)\n",
    "\n",
    "# Cria e treina o modelo Dummy\n",
    "bm = DummyClassifier() \n",
    "bm.fit(X_train, y_train)\n",
    "\n",
    "# Avalia a acurácia\n",
    "accuracy = bm.score(X_test, y_test) \n",
    "\n",
    "print(f\"Acurácia do modelo Dummy (Baseline): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b80439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Comparação de Modelos com Validação Cruzada (CORRIGIDA)\n",
    "\n",
    "# 1. Recombina os dados (necessário para a Validação Cruzada)\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])\n",
    "\n",
    "# 2. Importações de modelos (Garantindo que estão todos aqui)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import xgboost\n",
    "import inspect # Importa o módulo inspect para verificar os argumentos aceitos\n",
    "\n",
    "# 3. Lista de Modelos\n",
    "MODELS = [\n",
    "    DummyClassifier, \n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    KNeighborsClassifier,\n",
    "    GaussianNB,\n",
    "    SVC,\n",
    "    RandomForestClassifier,\n",
    "    xgboost.XGBClassifier,\n",
    "]\n",
    "\n",
    "# 4. Loop de Comparação de Modelos\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "print(f\"{'Modelo':22} {'AUC':5} {'STD':5}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "\n",
    "for model in MODELS:\n",
    "    # 5. Lógica de Inicialização: Verifica se o modelo aceita 'random_state'\n",
    "    \n",
    "    # Obtém os parâmetros que o construtor do modelo (o __init__) aceita\n",
    "    params = inspect.signature(model.__init__).parameters\n",
    "    \n",
    "    if 'random_state' in params:\n",
    "        # Inicializa o modelo passando random_state=42\n",
    "        cls = model(random_state=42)\n",
    "    else:\n",
    "        # Inicializa o modelo sem o argumento random_state\n",
    "        cls = model() \n",
    "    \n",
    "    # 6. Validação Cruzada\n",
    "    s = model_selection.cross_val_score(cls, X, y, scoring=\"roc_auc\", cv=kfold)\n",
    "    \n",
    "    # 7. Imprime o resultado\n",
    "    print(f\"{model.__name__:22} AUC: \"\n",
    "          f\"{s.mean():.3f} STD: {s.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b44191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7: Treinamento e Avaliação do Random Forest (Com n_estimators=10)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier # Importação limpa e direta\n",
    "\n",
    "# 1. Cria o objeto do modelo\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=10,  # Usando 10 estimadores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Treinamento do modelo\n",
    "rf.fit(X_train, y_train) \n",
    "\n",
    "# 3. Avaliação no conjunto de teste\n",
    "score = rf.score(X_test, y_test)\n",
    "\n",
    "# 4. Imprime o resultado\n",
    "print(f\"Acurácia do RandomForest (no conjunto de teste, com 10 árvores): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 8: Grid Search (Ajuste de Hiperparâmetros - TENTANDO REDUZIR OVERFITTING)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "# X_train e y_train devem estar carregados na memória\n",
    "\n",
    "# 1. Inicializa o modelo base (COM random_state fixo)\n",
    "rf4 = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 2. Define o dicionário de parâmetros a serem testados\n",
    "# NOVOS VALORES: Inclusão de max_depth e min_samples_leaf mais restritivos\n",
    "params = {\n",
    "    \"max_features\": [0.4, \"sqrt\"], \n",
    "    \"n_estimators\": [15, 200],\n",
    "    \n",
    "    # Valores mais restritivos para forçar a generalização (reduzir Overfitting)\n",
    "    \"min_samples_leaf\": [5, 10, 20], \n",
    "    \n",
    "    # NOVO PARÂMETRO: Limita a profundidade das árvores (muito eficaz contra Overfitting)\n",
    "    \"max_depth\": [5, 8, None] # 5 e 8 são limites, None deixa ir até o fim\n",
    "} \n",
    "\n",
    "# 3. Executa o Grid Search\n",
    "cv = model_selection.GridSearchCV(\n",
    "    rf4, \n",
    "    params, \n",
    "    n_jobs=-1, \n",
    "    scoring=\"accuracy\", # Usando 'accuracy' como métrica, como no original\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# 4. Imprime os melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros encontrados (tentando reduzir overfitting):\")\n",
    "print(cv.best_params_)\n",
    "\n",
    "# 5. Armazena o melhor modelo encontrado\n",
    "rf5 = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 9: Treinamento e Avaliação do RF Otimizado\n",
    "\n",
    "# 1. Cria o objeto do modelo usando os melhores parâmetros do Grid Search\n",
    "# O operador ** desempacota o dicionário cv.best_params_ em argumentos nomeados\n",
    "# Isso garante que você use a combinação {'max_features': 'auto', 'min_samples_leaf': 0.1, 'n_estimators': 200}\n",
    "rf5 = RandomForestClassifier(\n",
    "    **cv.best_params_, \n",
    "    random_state=42 # Adiciona o random_state para reprodutibilidade\n",
    ")\n",
    "\n",
    "# 2. Treinamento\n",
    "rf5.fit(X_train, y_train)\n",
    "\n",
    "# 3. Avaliação no conjunto de teste\n",
    "score = rf5.score(X_test, y_test)\n",
    "\n",
    "# 4. Imprime o resultado\n",
    "print(f\"Acurácia do Random Forest Otimizado: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff798306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativa: Usando apenas Scikit-learn (Não depende de distutils)\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y_pred = rf5.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=[\"Morreu (0)\", \"Sobreviveu (1)\"]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "plt.title(\"Matriz de Confusão (Random Forest Otimizado)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe151e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred = rf5.predict(X_test)\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 12: Visualização da Curva ROC (Usando Scikit-learn e Matplotlib)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Obter as Probabilidades\n",
    "# O ROC-AUC precisa da probabilidade de ser a classe positiva (Sobreviveu=1).\n",
    "# O [:, 1] seleciona a coluna de probabilidade para a classe 1.\n",
    "y_proba = rf5.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. CALCULA A CURVA ROC\n",
    "# 'fpr' (False Positive Rate) e 'tpr' (True Positive Rate) são os eixos do gráfico.\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# 3. CALCULA A ÁREA SOB A CURVA (AUC)\n",
    "# Este é o valor numérico que você já calculou (0.7888...)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# 4. PLOTA O GRÁFICO\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plota a Curva ROC do modelo\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'Curva ROC (área = {roc_auc:.4f})')\n",
    "\n",
    "# Plota a linha de referência (classificador aleatório, 50% de chance)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Aleatório') \n",
    "\n",
    "plt.xlabel('Taxa de Falsos Positivos (FPR)')\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos (TPR)')\n",
    "plt.title('Curva ROC para Random Forest Otimizado')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Você pode salvar o arquivo aqui, se desejar.\n",
    "plt.savefig(\"Curva_ROC_rf5.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 13: Curva de Aprendizado (Usando Scikit-learn e Matplotlib)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve, StratifiedKFold\n",
    "\n",
    "# X e y devem ser os dados de treino completos (antes de X_train/X_test)\n",
    "# Usando X e y completos, como o Yellowbrick faz, para avaliar o modelo RF5\n",
    "\n",
    "# 1. Define o número de folds e o tamanho das amostras de treino\n",
    "cv = StratifiedKFold(n_splits=12, shuffle=True, random_state=42)\n",
    "train_sizes = np.linspace(0.3, 1.0, 10)\n",
    "\n",
    "# 2. Calcula as pontuações da curva de aprendizado\n",
    "train_sizes_abs, train_scores, test_scores = learning_curve(\n",
    "    rf5,                 # O modelo otimizado\n",
    "    X,                   # Conjunto de dados COMPLETO (X)\n",
    "    y,                   # Rótulos COMPLETOS (y)\n",
    "    cv=cv,\n",
    "    train_sizes=train_sizes,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,           # Usa todos os cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Calcula as médias e desvios padrão para plotagem\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# 4. PLOTA A CURVA DE APRENDIZADO\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Curva de Aprendizado do Random Forest (F1 Score)\")\n",
    "plt.xlabel(\"Tamanho do Conjunto de Treino\")\n",
    "plt.ylabel(\"Score (F1 Ponderado)\")\n",
    "\n",
    "# Preenche a área de variação do treino\n",
    "plt.fill_between(train_sizes_abs, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "\n",
    "# Preenche a área de variação da validação (teste)\n",
    "plt.fill_between(train_sizes_abs, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "# Plota as médias\n",
    "plt.plot(train_sizes_abs, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Score de Treino\")\n",
    "plt.plot(train_sizes_abs, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Score de Validação Cruzada\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.savefig(\"Curva_Aprendizado_rf5.png\") # Salva o arquivo\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from  sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "cv = StratifiedKFold(12)\n",
    "sizes = np.linspace(0.3, 1.0, 10)\n",
    "lc_viz = LearningCurve(\n",
    "    rf5,\n",
    "    cv = cv,\n",
    "    train_sizes=sizes,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=4,\n",
    "    ax=ax,\n",
    ")\n",
    "plt.gcf().savefig(\"titanic/images/mlpr_0306.png\", dpi=300, bbox_inches=\"tight\")\n",
    "lc_viz.fit(X, y)\n",
    "lc_viz.poof()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import janitor as jn\n",
    "import pandas as pd\n",
    "\n",
    "Xbad = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [1, None, 3],\n",
    "        \"  sales_numbers  \": [20.0, 30.0, None],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dada32",
   "metadata": {},
   "outputs": [],
   "source": [
    "jn.clean_names(Xbad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37893d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_col(name):\n",
    "    return(\n",
    "        name.strip().lower().replace(\" \", \"_\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81705ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xbad.rename(columns=clean_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f94f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xbad.fillna(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb023181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
